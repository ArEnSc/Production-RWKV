{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lm-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prwkv.rwkvtokenizer import RWKVTokenizer\n",
    "from prwkv.rwkvrnnmodel import RWKVRNN4NeoForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval.models.gpt2 import GPT2LM\n",
    "from lm_eval import tasks, evaluator\n",
    "import math\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7' # CHANGE ME!\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "RWKV_PAD = [0] # <|endoftext|>\n",
    "# RWKV_PAD = [187] # \\n\n",
    "# RWKV_PAD = [187, 187] # \\n\\n\n",
    "\n",
    "RUN_TABLE = [1652] # part of model file name\n",
    "RUN_MODEL_NAME = '/mnt/ssd-1/BlinkDL_dont_delete/B/TRAIN_100M/out/all-'\n",
    "\n",
    "eval_tasks=['lambada','hellaswag','piqa']\n",
    "# eval_tasks=['hellaswag']\n",
    "# eval_tasks=['piqa']\n",
    "\n",
    "TEST_MODEL = 'rwkv' \n",
    "USE_CUDA = True # True False\n",
    "RUN_DEVICE = 'cuda' if USE_CUDA else 'cpu' # cpu cuda\n",
    "######### Set RUN_DEVICE in src/model.py too !!!\n",
    "\n",
    "RWKV_SLOW_MODE = True # True False\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EvalHarnessAdapter(GPT2LM):\n",
    "    def __init__(self,tokenizer=None,rwkv_rnn=None,rwkv_gpt=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.logitBuf = {}\n",
    "        self.correctBuf = {}\n",
    "        self.rwkv_rnn = rwkv_rnn\n",
    "        self.rwkv_gpt = rwkv_gpt\n",
    "\n",
    "    def greedy_until(self, requests):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _loglikelihood_tokens(self, requests, disable_tqdm=False):\n",
    "        res = []\n",
    "        sum_logit = 0\n",
    "        nCorrect = 0\n",
    "\n",
    "        for COUNTER in range(len(requests)):\n",
    "            n = COUNTER\n",
    "\n",
    "            raw_src = requests[n][0][0] + requests[n][0][1]\n",
    "\n",
    "            src = requests[n][1] + requests[n][2]\n",
    "            if TEST_MODEL == 'rwkv':\n",
    "                raw_src = '\\n' + raw_src\n",
    "                src = RWKV_PAD + src\n",
    "\n",
    "            sss = str(src)\n",
    "            correct = True\n",
    "            if sss in self.logitBuf:\n",
    "                logit = self.logitBuf[sss]\n",
    "                correct = self.correctBuf[sss]\n",
    "            else:\n",
    "                q_len = len(requests[n][1])\n",
    "                if TEST_MODEL == 'rwkv':\n",
    "                    q_len += len(RWKV_PAD)\n",
    "                logit = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if self.rwkv_rnn !=None:\n",
    "                        rwkv_rnn.clear()\n",
    "                        for i in range(1, len(src)):\n",
    "                            x = src[:i]\n",
    "                            out = rwkv_rnn.run(x)\n",
    "                            if i >= q_len:\n",
    "                                oo = torch.tensor(out)\n",
    "                                sorted_probs, s_index = torch.sort(oo, descending=True)\n",
    "                                pred = s_index[0].item()\n",
    "                                if pred != src[i]:\n",
    "                                    correct = False\n",
    "                                # print(x, '=>', src[i], 'pred', pred)\n",
    "                                logit += math.log(F.softmax(oo, dim=-1)[src[i]])\n",
    "                self.logitBuf[sss] = logit\n",
    "                self.correctBuf[sss] = correct\n",
    "            \n",
    "            if correct:\n",
    "                nCorrect += 1\n",
    "            res += [(logit, correct)]\n",
    "            sum_logit += logit\n",
    "            mean = sum_logit / (COUNTER+1)\n",
    "            acc = nCorrect / (COUNTER+1) * 100\n",
    "\n",
    "            if n % 100 == 0:\n",
    "                print(f'{n//100}/{len(requests)//100}', end = ' ', flush=True)\n",
    "        return res\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_eval(self, eval_tasks=None, num_fewshot=0, bootstrap_iters=2):\n",
    "        results = evaluator.evaluate(\n",
    "            lm=self,\n",
    "            task_dict=tasks.get_task_dict(eval_tasks),\n",
    "            provide_description=False,\n",
    "            num_fewshot=num_fewshot,\n",
    "            limit=None,\n",
    "            bootstrap_iters=bootstrap_iters,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "class LMEvaluationRunner():\n",
    "    def main():\n",
    "        tokenizer = RWKVTokenizer.default()\n",
    "        model = RWKVRNN4NeoForCausalLM.from_pretrained(\"/Users/michaelchung/Code/Production-RWKV/RWKV-4-Pile-430M-20220808-8066\",number_of_layers=24,embedding_dimension=1024,context_length=1024)\n",
    "           \n",
    "        print(\"Running evaluation harness...\")\n",
    "        adapter = EvalHarnessAdapter(tokenizer=tokenizer,rwkv_rnn=model)\n",
    "        results = adapter.run_eval(\n",
    "            eval_tasks=eval_tasks,\n",
    "            bootstrap_iters=10000,\n",
    "        )\n",
    "        print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('rwkvEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd6a94024d554874dbf5ff6d6423285372ef22e1c316548d6c117387ddc97bda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
